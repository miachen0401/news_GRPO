# GRPO Training Configuration
# This file contains all training hyperparameters
# Adjust these settings based on your GPU specifications

# ==============================================================================
# GPU Configuration Presets
# ==============================================================================
# Model Configuration
model:
  path: "Qwen/Qwen2.5-1.5B-Instruct"  # HuggingFace model path, "Qwen/Qwen2.5-0.5B-Instruct"
  use_remove_padding: true  # Enable for Flash Attention 2 (A100/H100 recommended)
  enable_gradient_checkpointing: true  # Save memory
  attn_implementation: "flash_attention_2"  # Use Flash Attention 2 for better performance

# Data Configuration
data:
  train_files: "data/gsm8k/train.parquet"
  val_files: "data/gsm8k/val.parquet"  # Validation split from training data
  train_batch_size: 4  # Increase for larger GPUs (A100: 16, 3090: 4)
  val_batch_size: 1
  max_prompt_length: 512  # Max input sequence length
  max_response_length: 256  # Max output sequence length
  dataloader_num_workers: 4  # Parallel data loading (adjust based on CPU cores)

# Rollout Configuration (Generation)
rollout:
  name: "vllm"  # Use vLLM for fast inference
  tensor_model_parallel_size: 1  # Multi-GPU: increase for model parallelism
  dtype: "float16"  # V100 optimized (use bfloat16 for A100)
  n: 4  # Number of samples per prompt (A100: 16, 3090: 4)
  temperature: 0.8  # Sampling temperature for generation
  do_sample: true # Enable sampling
  log_prob_micro_batch_size: 2 # Batch size for log prob calculation
  gpu_memory_utilization: 0.45  # Target GPU memory usage (A100: 0.90, 3090: 0.80)

# Actor Training Configuration (Policy Optimization)
actor:
  learning_rate: 1.0e-6  # Learning rate for actor model
  ppo_mini_batch_size: 1  # Total mini-batch size (A100: 32, 3090: 8)
  ppo_micro_batch_size_per_gpu: 1  # Per-GPU micro-batch (adjust for memory)
  ppo_epochs: 1  # PPO epochs per batch (GRPO typically uses 1)

# Algorithm Configuration
algorithm:
  adv_estimator: "grpo"  # Use GRPO (Group Relative Policy Optimization)

# Reward Function Configuration
reward:
  path: "GRPO/gsm8k_reward_partial.py" #"GRPO/gsm8k_reward_partial.py"
  function_name: "compute_score"

# Training Configuration
trainer:
  n_gpus_per_node: 1  # Number of GPUs to use
  nnodes: 1  # Number of nodes (for distributed training)
  total_epochs: 3  # Number of training epochs
  save_freq: 0  # Save checkpoint every N steps

  # Validation settings
  val_before_train: false  # Run validation before training starts
  val_interval: 0  # Run validation every N steps

  # Optimization settings
  mixed_precision: "fp16"  # Mixed precision training (fp16 or bf16)
  max_grad_norm: 1.0  # Gradient clipping threshold

  # Logging
  logger: ['console', 'wandb']  # Logger types (console and wandb)
  project_name: "qwen-grpo-gsm8k"
  experiment_name: "qwen-grpo-gsm8k"
  default_local_dir: "checkpoints/qwen-grpo-gsm8k"

  # Wandb configuration
  wandb:
    enabled: true
    project: "qwen-grpo-gsm8k"
    entity: ~  # Set your wandb username/team here
    tags: ["grpo", "gsm8k", "qwen3-4b", "v100"]
    notes: "GRPO training on GSM8K with Qwen3 4B"
    # Key metrics to monitor:
    # - critic/score/mean: Model correctness (training objective)
    # - critic/rewards/mean: Whether reward is being learned
    # - actor/entropy: Exploration level (decreases during training)
    # - actor/ppo_kl: KL divergence (too large=divergence, 0=no update)
    # - perf/throughput: Training throughput
    # - timing_s/update_actor: Update time (slow=memory issue or deadlock)
    # - response_length: Output length (too long=rambling, too short=collapse)

# Environment Variables
environment:
  pytorch_alloc_conf: "expandable_segments:True"
  tokenizers_parallelism: "true"
  # Reward logging configuration
  reward_log_sample_rate: "25"  # Log every 25 questions for monitoring
  reward_enable_logging: "true"  # Enable detailed logging of samples